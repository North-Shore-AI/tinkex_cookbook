#!/usr/bin/env python3
"""
Compare parity artifacts between Python and Elixir implementations.

This script compares the artifacts generated by the parity mode runs
of both Python and Elixir sl_basic recipes and produces a summary report.

Usage:
    python scripts/parity/compare_artifacts.py [python_dir] [elixir_dir]

Defaults:
    python_dir: /tmp/parity/sl_basic/python
    elixir_dir: /tmp/parity/sl_basic/elixir

Output:
    - Comparison summary printed to stdout
    - JSON report written to parent directory (e.g., /tmp/parity/sl_basic/report.json)
"""

import argparse
import json
import os
import sys
from dataclasses import dataclass, field
from typing import Any


@dataclass
class ComparisonResult:
    """Result of comparing two artifacts."""

    name: str
    status: str  # "match", "mismatch", "python_only", "elixir_only", "error"
    details: dict = field(default_factory=dict)
    python_value: Any = None
    elixir_value: Any = None


def load_json_artifact(path: str) -> dict | None:
    """Load a JSON artifact file."""
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception as e:
        return {"error": str(e)}


def compare_configs(python_dir: str, elixir_dir: str) -> ComparisonResult:
    """Compare config.json files."""
    python_config = load_json_artifact(os.path.join(python_dir, "config.json"))
    elixir_config = load_json_artifact(os.path.join(elixir_dir, "config.json"))

    if python_config is None and elixir_config is None:
        return ComparisonResult("config", "error", {"message": "Both configs missing"})
    if python_config is None:
        return ComparisonResult("config", "python_only", elixir_value=elixir_config)
    if elixir_config is None:
        return ComparisonResult("config", "elixir_only", python_value=python_config)

    # Compare key fields
    key_fields = ["model_name", "learning_rate", "lr_schedule", "num_epochs", "batch_size", "max_length"]
    differences = {}

    for field_name in key_fields:
        py_val = python_config.get(field_name)
        ex_val = elixir_config.get(field_name)
        if py_val != ex_val:
            differences[field_name] = {"python": py_val, "elixir": ex_val}

    if differences:
        return ComparisonResult(
            "config",
            "mismatch",
            {"differences": differences},
            python_config,
            elixir_config,
        )
    return ComparisonResult("config", "match", {"matched_fields": key_fields})


def compare_metrics(python_dir: str, elixir_dir: str) -> ComparisonResult:
    """Compare metrics.jsonl files."""
    python_path = os.path.join(python_dir, "metrics.jsonl")
    elixir_path = os.path.join(elixir_dir, "metrics.jsonl")

    def load_jsonl(path: str) -> list[dict]:
        if not os.path.exists(path):
            return []
        lines = []
        with open(path, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        lines.append(json.loads(line))
                    except json.JSONDecodeError:
                        pass
        return lines

    python_metrics = load_jsonl(python_path)
    elixir_metrics = load_jsonl(elixir_path)

    if not python_metrics and not elixir_metrics:
        return ComparisonResult("metrics", "error", {"message": "Both metrics files empty/missing"})
    if not python_metrics:
        return ComparisonResult("metrics", "python_only", elixir_value={"line_count": len(elixir_metrics)})
    if not elixir_metrics:
        return ComparisonResult("metrics", "elixir_only", python_value={"line_count": len(python_metrics)})

    details = {
        "python_line_count": len(python_metrics),
        "elixir_line_count": len(elixir_metrics),
    }

    # Compare step counts
    python_steps = set(m.get("step") for m in python_metrics if "step" in m)
    elixir_steps = set(m.get("step") for m in elixir_metrics if "step" in m)

    details["python_steps"] = sorted(list(python_steps))
    details["elixir_steps"] = sorted(list(elixir_steps))
    details["steps_match"] = python_steps == elixir_steps

    if len(python_metrics) != len(elixir_metrics):
        return ComparisonResult("metrics", "mismatch", details)

    return ComparisonResult("metrics", "match" if details["steps_match"] else "mismatch", details)


def compare_dataset_snapshot(python_dir: str, elixir_dir: str) -> ComparisonResult:
    """Compare dataset_snapshot.json files."""
    python_parity = os.path.join(python_dir, "parity")
    elixir_parity = os.path.join(elixir_dir, "parity")

    python_snapshot = load_json_artifact(os.path.join(python_parity, "dataset_snapshot.json"))
    elixir_snapshot = load_json_artifact(os.path.join(elixir_parity, "dataset_snapshot.json"))

    if python_snapshot is None and elixir_snapshot is None:
        return ComparisonResult("dataset_snapshot", "error", {"message": "Both snapshots missing"})
    if python_snapshot is None:
        return ComparisonResult("dataset_snapshot", "python_only", elixir_value=elixir_snapshot)
    if elixir_snapshot is None:
        return ComparisonResult("dataset_snapshot", "elixir_only", python_value=python_snapshot)

    details = {
        "python_total_count": python_snapshot.get("total_count"),
        "elixir_total_count": elixir_snapshot.get("total_count"),
        "python_n_snapshot": python_snapshot.get("n_snapshot"),
        "elixir_n_snapshot": elixir_snapshot.get("n_snapshot"),
    }

    # Compare sample hashes
    python_samples = python_snapshot.get("samples", [])
    elixir_samples = elixir_snapshot.get("samples", [])

    python_hashes = [s.get("content_hash") for s in python_samples]
    elixir_hashes = [s.get("content_hash") for s in elixir_samples]

    details["hash_matches"] = python_hashes == elixir_hashes
    details["python_hashes"] = python_hashes[:5]  # First 5
    details["elixir_hashes"] = elixir_hashes[:5]

    if details["hash_matches"]:
        return ComparisonResult("dataset_snapshot", "match", details)
    return ComparisonResult(
        "dataset_snapshot",
        "mismatch",
        details,
        python_snapshot,
        elixir_snapshot,
    )


def compare_rendered_samples(python_dir: str, elixir_dir: str) -> ComparisonResult:
    """Compare rendered_samples.json files."""
    python_parity = os.path.join(python_dir, "parity")
    elixir_parity = os.path.join(elixir_dir, "parity")

    python_rendered = load_json_artifact(os.path.join(python_parity, "rendered_samples.json"))
    elixir_rendered = load_json_artifact(os.path.join(elixir_parity, "rendered_samples.json"))

    if python_rendered is None and elixir_rendered is None:
        return ComparisonResult("rendered_samples", "error", {"message": "Both rendered samples missing"})
    if python_rendered is None:
        return ComparisonResult("rendered_samples", "python_only", elixir_value=elixir_rendered)
    if elixir_rendered is None:
        return ComparisonResult("rendered_samples", "elixir_only", python_value=python_rendered)

    python_samples = python_rendered.get("samples", [])
    elixir_samples = elixir_rendered.get("samples", [])

    details = {
        "python_count": len(python_samples),
        "elixir_count": len(elixir_samples),
    }

    # Compare token counts and weights
    if python_samples and elixir_samples:
        token_matches = []
        weight_matches = []

        for i, (py_s, ex_s) in enumerate(zip(python_samples, elixir_samples)):
            py_tokens = py_s.get("all_tokens", [])
            ex_tokens = ex_s.get("all_tokens", [])
            token_matches.append(py_tokens == ex_tokens)

            py_weights = py_s.get("weights", [])
            ex_weights = ex_s.get("weights", [])
            weight_matches.append(py_weights == ex_weights)

        details["token_matches"] = token_matches
        details["weight_matches"] = weight_matches
        details["all_tokens_match"] = all(token_matches)
        details["all_weights_match"] = all(weight_matches)

        # Compare first sample token counts
        if python_samples:
            details["first_sample_python_tokens"] = python_samples[0].get("token_count")
        if elixir_samples:
            details["first_sample_elixir_tokens"] = elixir_samples[0].get("token_count")

    all_match = details.get("all_tokens_match", False) and details.get("all_weights_match", False)
    return ComparisonResult(
        "rendered_samples",
        "match" if all_match else "mismatch",
        details,
        python_rendered,
        elixir_rendered,
    )


def compare_first_batch_payload(python_dir: str, elixir_dir: str) -> ComparisonResult:
    """Compare first_batch_payload.json files."""
    python_parity = os.path.join(python_dir, "parity")
    elixir_parity = os.path.join(elixir_dir, "parity")

    python_payload = load_json_artifact(os.path.join(python_parity, "first_batch_payload.json"))
    elixir_payload = load_json_artifact(os.path.join(elixir_parity, "first_batch_payload.json"))

    if python_payload is None and elixir_payload is None:
        return ComparisonResult("first_batch_payload", "error", {"message": "Both payloads missing"})
    if python_payload is None:
        return ComparisonResult("first_batch_payload", "python_only", elixir_value=elixir_payload)
    if elixir_payload is None:
        return ComparisonResult("first_batch_payload", "elixir_only", python_value=python_payload)

    details = {
        "python_batch_size": python_payload.get("batch_size"),
        "elixir_batch_size": elixir_payload.get("batch_size"),
        "python_payload_hash": python_payload.get("payload_hash"),
        "elixir_payload_hash": elixir_payload.get("payload_hash"),
    }

    details["hash_match"] = (
        python_payload.get("payload_hash") == elixir_payload.get("payload_hash")
    )

    if details["hash_match"]:
        return ComparisonResult("first_batch_payload", "match", details)
    return ComparisonResult(
        "first_batch_payload",
        "mismatch",
        details,
        python_payload,
        elixir_payload,
    )


def run_comparison(python_dir: str, elixir_dir: str) -> dict:
    """Run full comparison and return results."""
    results = []

    # Run all comparisons
    results.append(compare_configs(python_dir, elixir_dir))
    results.append(compare_metrics(python_dir, elixir_dir))
    results.append(compare_dataset_snapshot(python_dir, elixir_dir))
    results.append(compare_rendered_samples(python_dir, elixir_dir))
    results.append(compare_first_batch_payload(python_dir, elixir_dir))

    # Build summary
    summary = {
        "python_dir": python_dir,
        "elixir_dir": elixir_dir,
        "results": [],
        "overall_status": "pass",
    }

    for result in results:
        result_dict = {
            "name": result.name,
            "status": result.status,
            "details": result.details,
        }
        summary["results"].append(result_dict)

        # Update overall status
        if result.status not in ("match", "python_only", "elixir_only"):
            summary["overall_status"] = "fail"

    # Count by status
    status_counts = {}
    for result in results:
        status_counts[result.status] = status_counts.get(result.status, 0) + 1
    summary["status_counts"] = status_counts

    return summary


def print_summary(summary: dict):
    """Print a human-readable summary."""
    print("=" * 60)
    print("PARITY COMPARISON REPORT")
    print("=" * 60)
    print()
    print(f"Python directory: {summary['python_dir']}")
    print(f"Elixir directory: {summary['elixir_dir']}")
    print()

    # Status counts
    print("Status Summary:")
    for status, count in sorted(summary["status_counts"].items()):
        symbol = "✓" if status == "match" else "✗" if status == "mismatch" else "?"
        print(f"  {symbol} {status}: {count}")
    print()

    # Individual results
    print("Detailed Results:")
    print("-" * 60)
    for result in summary["results"]:
        status_symbol = {
            "match": "✓",
            "mismatch": "✗",
            "python_only": "P",
            "elixir_only": "E",
            "error": "!",
        }.get(result["status"], "?")

        print(f"\n{status_symbol} {result['name']}: {result['status']}")

        # Print key details
        details = result.get("details", {})
        for key, value in details.items():
            if key in ("differences", "message"):
                print(f"    {key}: {value}")
            elif key.endswith("_match") or key.endswith("_matches"):
                print(f"    {key}: {value}")
            elif key.endswith("_count"):
                print(f"    {key}: {value}")
            elif key == "matched_fields":
                print(f"    fields checked: {', '.join(value)}")

    print()
    print("-" * 60)
    overall = "PASS" if summary["overall_status"] == "pass" else "FAIL"
    print(f"Overall: {overall}")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="Compare parity artifacts between Python and Elixir implementations."
    )
    parser.add_argument(
        "python_dir",
        nargs="?",
        default="/tmp/parity/sl_basic/python",
        help="Path to Python artifacts directory",
    )
    parser.add_argument(
        "elixir_dir",
        nargs="?",
        default="/tmp/parity/sl_basic/elixir",
        help="Path to Elixir artifacts directory",
    )
    parser.add_argument(
        "--json",
        "-j",
        action="store_true",
        help="Output raw JSON instead of formatted report",
    )
    parser.add_argument(
        "--output",
        "-o",
        help="Output path for JSON report (default: parent of python_dir/report.json)",
    )

    args = parser.parse_args()

    # Validate directories exist
    if not os.path.isdir(args.python_dir):
        print(f"Error: Python directory does not exist: {args.python_dir}", file=sys.stderr)
        sys.exit(1)
    if not os.path.isdir(args.elixir_dir):
        print(f"Error: Elixir directory does not exist: {args.elixir_dir}", file=sys.stderr)
        sys.exit(1)

    # Run comparison
    summary = run_comparison(args.python_dir, args.elixir_dir)

    # Output
    if args.json:
        print(json.dumps(summary, indent=2))
    else:
        print_summary(summary)

    # Write JSON report
    output_path = args.output
    if not output_path:
        # Default: write to parent of python_dir
        output_path = os.path.join(os.path.dirname(args.python_dir), "report.json")

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(summary, f, indent=2)
    print(f"\nJSON report written to: {output_path}")

    # Exit with appropriate code
    sys.exit(0 if summary["overall_status"] == "pass" else 1)


if __name__ == "__main__":
    main()
